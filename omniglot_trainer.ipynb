{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import copy \n",
    "\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from torch.autograd import Variable \n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from dataloader import OmniClassDataset, OmniLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path('data/omniglot/')\n",
    "data_dir = base/'data'\n",
    "split_dir = base/'splits'/'vinyals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(split, k_shot, n_way, n_test):\n",
    "    dataset = OmniClassDataset(split=split,\n",
    "                           data_dir=data_dir, \n",
    "                           splits_dir=split_dir,\n",
    "                           shuffle=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               lambda x: 1 - x\n",
    "                           ]))\n",
    "    \n",
    "    dataloader = OmniLoader(k_shot=k_shot, \n",
    "                            n_way=n_way,\n",
    "                            n_test=n_test,\n",
    "                            dataset=dataset,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=8)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ran with 3 diff seeds \n",
    "# linearly annealed \n",
    "\n",
    "way5_params = {\n",
    "    'n_way': 5,\n",
    "    'k_shots': 1,\n",
    "    'n_test': 1,\n",
    "    # inner loop parameters\n",
    "    'inner_lr': 0.001,\n",
    "    'inner_batchsize': 10,\n",
    "    'inner_iterations': 5,\n",
    "    # outter loop parameters\n",
    "    'outer_lr': 1.0,\n",
    "    'outer_iterations': 100000,\n",
    "    'meta_batchsize': 5,\n",
    "    # evaluation params\n",
    "    'eval_inner_iterations': 50,\n",
    "    'eval_inner_batch': 5,\n",
    "    # other...\n",
    "    'validation_rate': 10\n",
    "}\n",
    "\n",
    "way20_params = {\n",
    "    'n_way': 20,\n",
    "    'k_shots': 1,\n",
    "    'n_test': 1,\n",
    "    # inner loop parameters\n",
    "    'inner_lr': 0.0005,\n",
    "    'inner_batchsize': 20,\n",
    "    'inner_iterations': 10,\n",
    "    # outter loop parameters\n",
    "    'outer_lr': 1.0,\n",
    "    'outer_iterations': 200000,\n",
    "    'meta_batchsize': 5,\n",
    "    # evaluation params\n",
    "    'eval_inner_iterations': 50,\n",
    "    'eval_inner_batch': 10,\n",
    "    # other...\n",
    "    'validation_rate': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv_block = lambda in_dim:(nn.Conv2d(in_dim, 64, 3, stride=2, padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU())\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            *conv_block(1),\n",
    "            *conv_block(64),\n",
    "            *conv_block(64), \n",
    "            *conv_block(64)\n",
    "        )\n",
    "        self.linear = nn.Linear(256, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = way5_params\n",
    "model = Model(n_classes=params['n_way'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outter_loop_optim = torch.optim.SGD(model.parameters(), lr=params['outer_lr'])\n",
    "inner_loop_optim = torch.optim.Adam(model.parameters(), lr=params['inner_lr'], betas=(0, 0))\n",
    "loss_fcn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_dataloader('train', params['k_shots'], params['n_way'], params['n_test'])\n",
    "val_dataloader = get_dataloader('val', params['k_shots'], params['n_way'], params['n_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n_steps(loss_fcn, optim, model, x, y, n_steps):\n",
    "    losses = []\n",
    "    for _ in range(n_steps):\n",
    "        optim.zero_grad()\n",
    "        loss = loss_fcn(model(x), y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_train(model, optim, loss_fcn, n_iterations, train_x, train_y, test_x, test_y):\n",
    "    task_model = copy.deepcopy(model)\n",
    "    inner_loop_opitm = torch.optim.Adam(task_model.parameters(), lr=params['inner_lr'], betas=(0, 0))\n",
    "\n",
    "    take_n_steps(loss_fcn, \n",
    "                 optim,\n",
    "                 task_model,\n",
    "                 train_x, train_y,\n",
    "                 n_iterations)\n",
    "\n",
    "    y_preds = task_model(test_x)\n",
    "    loss = loss_fcn(y_preds, test_y)\n",
    "    n_correct = (F.softmax(y_preds, dim=-1).argmax(-1) == test_y).sum()\n",
    "    accuracy = n_correct / test_y.size(0)\n",
    "    \n",
    "    return task_model, loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 2\n",
    "batch_size = 4\n",
    "k_shot = 2\n",
    "n_test = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gabrielhuang/reptile-pytorch/blob/master/train_omniglot.py\n",
    "def make_inf(D):\n",
    "    while True:\n",
    "        for x in D:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OmniClassDataset(split='train',\n",
    "                   data_dir=data_dir, \n",
    "                   splits_dir=split_dir,\n",
    "                   shuffle=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       lambda x: 1 - x\n",
    "                   ]))\n",
    "\n",
    "train_loader = make_inf(OmniLoader(k_shot=k_shot, \n",
    "                                    n_way=n_way,\n",
    "                                    n_test=n_test,\n",
    "                                    dataset=dataset,\n",
    "                                    shuffle=True,\n",
    "                                    pin_memory=True,\n",
    "                                    num_workers=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size):\n",
    "    x_batch = torch.stack(list(torch.chunk(x, batch_size)) if x.size(0) % batch_size == 0 else list(torch.chunk(x, batch_size+1))[:-1])\n",
    "    y_batch = torch.stack(list(torch.chunk(y, batch_size)) if x.size(0) % batch_size == 0 else list(torch.chunk(y, batch_size+1))[:-1])\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, n_test):\n",
    "    test_idxs, train_idxs = [], []\n",
    "    \n",
    "    for class_i in range(int(y.max())):\n",
    "        class_ex = (y == class_i).nonzero().flatten()\n",
    "        class_ex = class_ex[torch.randperm(class_ex.size(0))]\n",
    "        ctest_idx = class_ex[:n_test]\n",
    "        ctrain_idx = class_ex[n_test:]\n",
    "\n",
    "        test_idxs.append(ctest_idx)\n",
    "        train_idxs.append(ctrain_idx)\n",
    "    \n",
    "    train_idxs = torch.cat(train_idxs)\n",
    "    x_train, y_train = x[train_idxs], y[train_idxs]\n",
    "    \n",
    "    test_idxs = torch.cat(test_idxs)\n",
    "    x_test, y_test = x[test_idxs], y[test_idxs]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 28, 28])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x, y in val_loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:01<02:13,  1.35s/it]\u001b[ATraceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/brennangebotys/miniconda2/envs/meta/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c06ae5694429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m                              \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                              \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                              params['eval_inner_iterations'])\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fa38b10d03ad>\u001b[0m in \u001b[0;36mtake_n_steps\u001b[0;34m(loss_fcn, optim, model, x, y, n_steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/meta/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/meta/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = '5w1s - 1test - full_run'\n",
    "writer = SummaryWriter(comment=model_name)\n",
    "debug = False\n",
    "\n",
    "params = way5_params\n",
    "model = Model(n_classes=params['n_way']).to(device)\n",
    "outter_loop_optim = torch.optim.SGD(model.parameters(), lr=params['outer_lr'])\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# debugging parameters\n",
    "if debug:\n",
    "    params['outer_iterations'] = 100\n",
    "    params['validation_rate'] = 3\n",
    "    params['meta_batchsize'] = 1\n",
    "    params['inner_batchsize'] = 15\n",
    "\n",
    "# params['outer_iterations'] = 500\n",
    "\n",
    "train_loader = get_dataloader('train', params['k_shots'], params['n_way'], params['n_test'])\n",
    "val_loader = get_dataloader('val', params['k_shots'], params['n_way'], params['n_test'])\n",
    "    \n",
    "for outer_i in tqdm(range(params['outer_iterations'])):\n",
    "    outter_loop_optim.zero_grad()\n",
    "    \n",
    "    # lr annealing \n",
    "    frac_done = outer_i / params['outer_iterations']\n",
    "    cur_meta_step_size = frac_done * 1e-9 + (1 - frac_done) * params['outer_lr']\n",
    "    for param_group in outter_loop_optim.param_groups:\n",
    "        param_group['lr'] = cur_meta_step_size\n",
    "    \n",
    "    # inner loop\n",
    "    for task_i, (x, y) in enumerate(train_loader):\n",
    "        new_model = model.clone()\n",
    "        inner_loop_opitm = torch.optim.Adam(new_model.parameters(), lr=params['inner_lr'], betas=(0, 0))\n",
    "        \n",
    "        # train on batches of the dataset\n",
    "        batch_x, batch_y = batch(x, y, batch_size=params['inner_batchsize'])\n",
    "        for i, (train_x, train_y) in enumerate(zip(batch_x, batch_y)):\n",
    "            take_n_steps(loss_fcn, \n",
    "                         inner_loop_opitm,\n",
    "                         new_model,\n",
    "                         train_x, train_y, 1)\n",
    "            if i == params['inner_iterations'] -1: \n",
    "              break\n",
    "        \n",
    "        # record weights\n",
    "        for w, w_t in zip(model.parameters(), new_model.parameters()):\n",
    "            if w.grad is None:\n",
    "                w.grad = Variable(torch.zeros_like(w)).to(device)\n",
    "            # invert loss eqn. to use descent optimization\n",
    "            w.grad.data.add_(w.data - w_t.data)\n",
    "        \n",
    "        if task_i == params['meta_batchsize'] - 1:\n",
    "            break\n",
    "\n",
    "    # eval \n",
    "    y_preds = new_model(x)\n",
    "    loss = loss_fcn(y_preds, y)\n",
    "    accuracy = (y_preds.argmax(-1) == y).float().mean()\n",
    "                        \n",
    "    writer.add_scalar('meta_train_loss', loss, outer_i)\n",
    "    writer.add_scalar('meta_train_acc', accuracy, outer_i)\n",
    "    \n",
    "    # update model with avg over mini batches \n",
    "    for w in model.parameters():\n",
    "        w.grad.data.div_(params['meta_batchsize'])\n",
    "    outter_loop_optim.step()\n",
    "    \n",
    "    # validation\n",
    "    if outer_i % params['validation_rate'] == 0:\n",
    "        \n",
    "        for (x, y) in val_loader:\n",
    "            new_model = model.clone()\n",
    "            inner_loop_opitm = torch.optim.Adam(new_model.parameters(), lr=params['inner_lr'], betas=(0, 0))\n",
    "            \n",
    "            (x_train, y_train), (x_test, y_test) = train_test_split(x, y, params['n_test'])\n",
    "            \n",
    "            # train\n",
    "            xb_train, yb_train = batch(x_train, y_train, batch_size=params['eval_inner_batch'])\n",
    "            for (train_x, train_y) in zip(xb_train, yb_train):\n",
    "                take_n_steps(loss_fcn, \n",
    "                             inner_loop_opitm,\n",
    "                             new_model,\n",
    "                             train_x, train_y,\n",
    "                             params['eval_inner_iterations'])\n",
    "            \n",
    "            # eval \n",
    "            y_preds = new_model(x_test)\n",
    "            loss = loss_fcn(y_preds, y_test)\n",
    "            accuracy = (y_preds.argmax(-1) == y_test).float().mean()\n",
    "                                \n",
    "            writer.add_scalar('meta_val_loss', loss, outer_i)\n",
    "            writer.add_scalar('meta_val_acc', accuracy, outer_i)\n",
    "            break\n",
    "        \n",
    "writer.close()\n",
    "print('Summary writer closed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_losses(losses, split_interval=100):\n",
    "    d = torch.split(torch.from_numpy(np.array(losses)), split_interval)\n",
    "    mean_losses = [x.mean() for x in d]\n",
    "    intervals = [split_interval * i for i in range(len(mean_losses))]\n",
    "    \n",
    "    plt.plot(intervals, mean_losses)\n",
    "    plt.title('Model Test Loss (averaged every {} iterations)'.format(split_interval))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_losses(val_loss, split_interval=params['eval_inner_batch'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
